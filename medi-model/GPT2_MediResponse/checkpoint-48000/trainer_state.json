{
  "best_metric": 0.7582518458366394,
  "best_model_checkpoint": "./GPT2_MediResponse/checkpoint-48000",
  "epoch": 13.241379310344827,
  "eval_steps": 16000,
  "global_step": 48000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.13793103448275862,
      "grad_norm": 14.004721641540527,
      "learning_rate": 3.075e-06,
      "loss": 30.1432,
      "step": 500
    },
    {
      "epoch": 0.27586206896551724,
      "grad_norm": 8.173078536987305,
      "learning_rate": 6.200000000000001e-06,
      "loss": 1.8538,
      "step": 1000
    },
    {
      "epoch": 0.41379310344827586,
      "grad_norm": 8.921845436096191,
      "learning_rate": 9.325000000000001e-06,
      "loss": 1.3839,
      "step": 1500
    },
    {
      "epoch": 0.5517241379310345,
      "grad_norm": 6.306473731994629,
      "learning_rate": 9.93049645390071e-06,
      "loss": 1.2298,
      "step": 2000
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 7.199012279510498,
      "learning_rate": 9.841843971631206e-06,
      "loss": 1.1684,
      "step": 2500
    },
    {
      "epoch": 0.8275862068965517,
      "grad_norm": 5.391665458679199,
      "learning_rate": 9.753191489361703e-06,
      "loss": 1.1061,
      "step": 3000
    },
    {
      "epoch": 0.9655172413793104,
      "grad_norm": 7.642928600311279,
      "learning_rate": 9.664539007092199e-06,
      "loss": 1.0676,
      "step": 3500
    },
    {
      "epoch": 1.103448275862069,
      "grad_norm": 6.788722038269043,
      "learning_rate": 9.575886524822696e-06,
      "loss": 1.0361,
      "step": 4000
    },
    {
      "epoch": 1.2413793103448276,
      "grad_norm": 5.763134002685547,
      "learning_rate": 9.487234042553193e-06,
      "loss": 1.0057,
      "step": 4500
    },
    {
      "epoch": 1.3793103448275863,
      "grad_norm": 5.564180374145508,
      "learning_rate": 9.398581560283688e-06,
      "loss": 0.9861,
      "step": 5000
    },
    {
      "epoch": 1.5172413793103448,
      "grad_norm": 6.578742504119873,
      "learning_rate": 9.309929078014186e-06,
      "loss": 0.9779,
      "step": 5500
    },
    {
      "epoch": 1.6551724137931034,
      "grad_norm": 5.960573673248291,
      "learning_rate": 9.221276595744681e-06,
      "loss": 0.9577,
      "step": 6000
    },
    {
      "epoch": 1.793103448275862,
      "grad_norm": 5.167537689208984,
      "learning_rate": 9.132624113475178e-06,
      "loss": 0.9522,
      "step": 6500
    },
    {
      "epoch": 1.9310344827586206,
      "grad_norm": 5.16453218460083,
      "learning_rate": 9.043971631205675e-06,
      "loss": 0.9371,
      "step": 7000
    },
    {
      "epoch": 2.0689655172413794,
      "grad_norm": 4.755068302154541,
      "learning_rate": 8.95531914893617e-06,
      "loss": 0.9216,
      "step": 7500
    },
    {
      "epoch": 2.206896551724138,
      "grad_norm": 5.226436138153076,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.9052,
      "step": 8000
    },
    {
      "epoch": 2.344827586206897,
      "grad_norm": 4.878798007965088,
      "learning_rate": 8.778014184397163e-06,
      "loss": 0.8989,
      "step": 8500
    },
    {
      "epoch": 2.4827586206896552,
      "grad_norm": 4.312306880950928,
      "learning_rate": 8.68936170212766e-06,
      "loss": 0.8959,
      "step": 9000
    },
    {
      "epoch": 2.6206896551724137,
      "grad_norm": 4.79581356048584,
      "learning_rate": 8.600709219858157e-06,
      "loss": 0.89,
      "step": 9500
    },
    {
      "epoch": 2.7586206896551726,
      "grad_norm": 4.765413761138916,
      "learning_rate": 8.512056737588653e-06,
      "loss": 0.887,
      "step": 10000
    },
    {
      "epoch": 2.896551724137931,
      "grad_norm": 4.910890579223633,
      "learning_rate": 8.42340425531915e-06,
      "loss": 0.8834,
      "step": 10500
    },
    {
      "epoch": 3.0344827586206895,
      "grad_norm": 4.0744123458862305,
      "learning_rate": 8.334751773049647e-06,
      "loss": 0.8643,
      "step": 11000
    },
    {
      "epoch": 3.1724137931034484,
      "grad_norm": 4.712132930755615,
      "learning_rate": 8.246099290780142e-06,
      "loss": 0.8595,
      "step": 11500
    },
    {
      "epoch": 3.310344827586207,
      "grad_norm": 4.811495304107666,
      "learning_rate": 8.157446808510638e-06,
      "loss": 0.8473,
      "step": 12000
    },
    {
      "epoch": 3.4482758620689653,
      "grad_norm": 4.912704944610596,
      "learning_rate": 8.068794326241135e-06,
      "loss": 0.8536,
      "step": 12500
    },
    {
      "epoch": 3.586206896551724,
      "grad_norm": 4.643569469451904,
      "learning_rate": 7.980141843971632e-06,
      "loss": 0.8514,
      "step": 13000
    },
    {
      "epoch": 3.7241379310344827,
      "grad_norm": 4.206403732299805,
      "learning_rate": 7.891489361702129e-06,
      "loss": 0.842,
      "step": 13500
    },
    {
      "epoch": 3.862068965517241,
      "grad_norm": 4.9271135330200195,
      "learning_rate": 7.802836879432624e-06,
      "loss": 0.8396,
      "step": 14000
    },
    {
      "epoch": 4.0,
      "grad_norm": 4.447263240814209,
      "learning_rate": 7.71436170212766e-06,
      "loss": 0.8323,
      "step": 14500
    },
    {
      "epoch": 4.137931034482759,
      "grad_norm": 3.948474168777466,
      "learning_rate": 7.625709219858156e-06,
      "loss": 0.8207,
      "step": 15000
    },
    {
      "epoch": 4.275862068965517,
      "grad_norm": 4.2563300132751465,
      "learning_rate": 7.537056737588653e-06,
      "loss": 0.818,
      "step": 15500
    },
    {
      "epoch": 4.413793103448276,
      "grad_norm": 3.885913133621216,
      "learning_rate": 7.44840425531915e-06,
      "loss": 0.8236,
      "step": 16000
    },
    {
      "epoch": 4.413793103448276,
      "eval_loss": 0.80704265832901,
      "eval_runtime": 51.8683,
      "eval_samples_per_second": 119.842,
      "eval_steps_per_second": 29.96,
      "step": 16000
    },
    {
      "epoch": 4.551724137931035,
      "grad_norm": 4.1239118576049805,
      "learning_rate": 7.3599290780141855e-06,
      "loss": 0.8163,
      "step": 16500
    },
    {
      "epoch": 4.689655172413794,
      "grad_norm": 3.7203099727630615,
      "learning_rate": 7.271276595744682e-06,
      "loss": 0.8143,
      "step": 17000
    },
    {
      "epoch": 4.827586206896552,
      "grad_norm": 3.9004249572753906,
      "learning_rate": 7.182624113475178e-06,
      "loss": 0.8099,
      "step": 17500
    },
    {
      "epoch": 4.9655172413793105,
      "grad_norm": 4.083950042724609,
      "learning_rate": 7.093971631205674e-06,
      "loss": 0.808,
      "step": 18000
    },
    {
      "epoch": 5.103448275862069,
      "grad_norm": 4.007016181945801,
      "learning_rate": 7.00549645390071e-06,
      "loss": 0.798,
      "step": 18500
    },
    {
      "epoch": 5.241379310344827,
      "grad_norm": 3.9441704750061035,
      "learning_rate": 6.916843971631206e-06,
      "loss": 0.794,
      "step": 19000
    },
    {
      "epoch": 5.379310344827586,
      "grad_norm": 3.6730337142944336,
      "learning_rate": 6.828191489361703e-06,
      "loss": 0.7928,
      "step": 19500
    },
    {
      "epoch": 5.517241379310345,
      "grad_norm": 3.9228129386901855,
      "learning_rate": 6.7395390070922e-06,
      "loss": 0.7961,
      "step": 20000
    },
    {
      "epoch": 5.655172413793103,
      "grad_norm": 4.684872150421143,
      "learning_rate": 6.650886524822695e-06,
      "loss": 0.7905,
      "step": 20500
    },
    {
      "epoch": 5.793103448275862,
      "grad_norm": 4.455144882202148,
      "learning_rate": 6.562411347517731e-06,
      "loss": 0.7938,
      "step": 21000
    },
    {
      "epoch": 5.931034482758621,
      "grad_norm": 4.033926486968994,
      "learning_rate": 6.473758865248227e-06,
      "loss": 0.7872,
      "step": 21500
    },
    {
      "epoch": 6.068965517241379,
      "grad_norm": 3.7047595977783203,
      "learning_rate": 6.385106382978724e-06,
      "loss": 0.7849,
      "step": 22000
    },
    {
      "epoch": 6.206896551724138,
      "grad_norm": 4.845690727233887,
      "learning_rate": 6.2964539007092205e-06,
      "loss": 0.7796,
      "step": 22500
    },
    {
      "epoch": 6.344827586206897,
      "grad_norm": 3.941082239151001,
      "learning_rate": 6.2079787234042555e-06,
      "loss": 0.7805,
      "step": 23000
    },
    {
      "epoch": 6.482758620689655,
      "grad_norm": 4.647071838378906,
      "learning_rate": 6.119326241134752e-06,
      "loss": 0.7755,
      "step": 23500
    },
    {
      "epoch": 6.620689655172414,
      "grad_norm": 3.9183506965637207,
      "learning_rate": 6.030673758865249e-06,
      "loss": 0.77,
      "step": 24000
    },
    {
      "epoch": 6.758620689655173,
      "grad_norm": 4.1123504638671875,
      "learning_rate": 5.942021276595745e-06,
      "loss": 0.7773,
      "step": 24500
    },
    {
      "epoch": 6.896551724137931,
      "grad_norm": 4.081951141357422,
      "learning_rate": 5.853368794326242e-06,
      "loss": 0.7683,
      "step": 25000
    },
    {
      "epoch": 7.0344827586206895,
      "grad_norm": 4.456040859222412,
      "learning_rate": 5.764893617021276e-06,
      "loss": 0.7684,
      "step": 25500
    },
    {
      "epoch": 7.172413793103448,
      "grad_norm": 4.084719657897949,
      "learning_rate": 5.676241134751773e-06,
      "loss": 0.7622,
      "step": 26000
    },
    {
      "epoch": 7.310344827586207,
      "grad_norm": 3.8879995346069336,
      "learning_rate": 5.58758865248227e-06,
      "loss": 0.7627,
      "step": 26500
    },
    {
      "epoch": 7.448275862068965,
      "grad_norm": 4.682872772216797,
      "learning_rate": 5.498936170212767e-06,
      "loss": 0.7609,
      "step": 27000
    },
    {
      "epoch": 7.586206896551724,
      "grad_norm": 3.509228467941284,
      "learning_rate": 5.4104609929078026e-06,
      "loss": 0.756,
      "step": 27500
    },
    {
      "epoch": 7.724137931034483,
      "grad_norm": 3.777012825012207,
      "learning_rate": 5.321808510638299e-06,
      "loss": 0.7554,
      "step": 28000
    },
    {
      "epoch": 7.862068965517241,
      "grad_norm": 3.8954317569732666,
      "learning_rate": 5.233156028368794e-06,
      "loss": 0.7592,
      "step": 28500
    },
    {
      "epoch": 8.0,
      "grad_norm": 5.155262470245361,
      "learning_rate": 5.144503546099291e-06,
      "loss": 0.762,
      "step": 29000
    },
    {
      "epoch": 8.137931034482758,
      "grad_norm": 3.576133966445923,
      "learning_rate": 5.056028368794327e-06,
      "loss": 0.7451,
      "step": 29500
    },
    {
      "epoch": 8.275862068965518,
      "grad_norm": 3.4544663429260254,
      "learning_rate": 4.9673758865248225e-06,
      "loss": 0.7441,
      "step": 30000
    },
    {
      "epoch": 8.413793103448276,
      "grad_norm": 3.74375581741333,
      "learning_rate": 4.87872340425532e-06,
      "loss": 0.7474,
      "step": 30500
    },
    {
      "epoch": 8.551724137931034,
      "grad_norm": 3.633392572402954,
      "learning_rate": 4.790070921985816e-06,
      "loss": 0.7473,
      "step": 31000
    },
    {
      "epoch": 8.689655172413794,
      "grad_norm": 3.506211996078491,
      "learning_rate": 4.701595744680852e-06,
      "loss": 0.7494,
      "step": 31500
    },
    {
      "epoch": 8.827586206896552,
      "grad_norm": 3.1771485805511475,
      "learning_rate": 4.612943262411348e-06,
      "loss": 0.7489,
      "step": 32000
    },
    {
      "epoch": 8.827586206896552,
      "eval_loss": 0.769010066986084,
      "eval_runtime": 56.1452,
      "eval_samples_per_second": 110.713,
      "eval_steps_per_second": 27.678,
      "step": 32000
    },
    {
      "epoch": 8.96551724137931,
      "grad_norm": 3.4604721069335938,
      "learning_rate": 4.524290780141844e-06,
      "loss": 0.7526,
      "step": 32500
    },
    {
      "epoch": 9.10344827586207,
      "grad_norm": 3.9859116077423096,
      "learning_rate": 4.4356382978723404e-06,
      "loss": 0.733,
      "step": 33000
    },
    {
      "epoch": 9.241379310344827,
      "grad_norm": 4.31630277633667,
      "learning_rate": 4.3469858156028375e-06,
      "loss": 0.7331,
      "step": 33500
    },
    {
      "epoch": 9.379310344827585,
      "grad_norm": 4.486700057983398,
      "learning_rate": 4.2585106382978725e-06,
      "loss": 0.7371,
      "step": 34000
    },
    {
      "epoch": 9.517241379310345,
      "grad_norm": 3.6922881603240967,
      "learning_rate": 4.169858156028369e-06,
      "loss": 0.7337,
      "step": 34500
    },
    {
      "epoch": 9.655172413793103,
      "grad_norm": 4.229286193847656,
      "learning_rate": 4.081205673758866e-06,
      "loss": 0.7402,
      "step": 35000
    },
    {
      "epoch": 9.793103448275861,
      "grad_norm": 3.8724939823150635,
      "learning_rate": 3.992553191489362e-06,
      "loss": 0.7381,
      "step": 35500
    },
    {
      "epoch": 9.931034482758621,
      "grad_norm": 3.9307193756103516,
      "learning_rate": 3.904078014184398e-06,
      "loss": 0.7376,
      "step": 36000
    },
    {
      "epoch": 10.068965517241379,
      "grad_norm": 4.691528797149658,
      "learning_rate": 3.815425531914894e-06,
      "loss": 0.7322,
      "step": 36500
    },
    {
      "epoch": 10.206896551724139,
      "grad_norm": 3.4760708808898926,
      "learning_rate": 3.7267730496453904e-06,
      "loss": 0.7265,
      "step": 37000
    },
    {
      "epoch": 10.344827586206897,
      "grad_norm": 4.01970911026001,
      "learning_rate": 3.6381205673758867e-06,
      "loss": 0.7257,
      "step": 37500
    },
    {
      "epoch": 10.482758620689655,
      "grad_norm": 3.7320990562438965,
      "learning_rate": 3.549645390070922e-06,
      "loss": 0.7289,
      "step": 38000
    },
    {
      "epoch": 10.620689655172415,
      "grad_norm": 3.925948143005371,
      "learning_rate": 3.4609929078014187e-06,
      "loss": 0.7326,
      "step": 38500
    },
    {
      "epoch": 10.758620689655173,
      "grad_norm": 4.899447441101074,
      "learning_rate": 3.3723404255319154e-06,
      "loss": 0.7311,
      "step": 39000
    },
    {
      "epoch": 10.89655172413793,
      "grad_norm": 3.428917407989502,
      "learning_rate": 3.2836879432624112e-06,
      "loss": 0.7199,
      "step": 39500
    },
    {
      "epoch": 11.03448275862069,
      "grad_norm": 4.12771463394165,
      "learning_rate": 3.195212765957447e-06,
      "loss": 0.726,
      "step": 40000
    },
    {
      "epoch": 11.172413793103448,
      "grad_norm": 3.939624547958374,
      "learning_rate": 3.1065602836879433e-06,
      "loss": 0.7182,
      "step": 40500
    },
    {
      "epoch": 11.310344827586206,
      "grad_norm": 3.7341222763061523,
      "learning_rate": 3.01790780141844e-06,
      "loss": 0.7177,
      "step": 41000
    },
    {
      "epoch": 11.448275862068966,
      "grad_norm": 4.123178958892822,
      "learning_rate": 2.9292553191489366e-06,
      "loss": 0.7199,
      "step": 41500
    },
    {
      "epoch": 11.586206896551724,
      "grad_norm": 3.9351236820220947,
      "learning_rate": 2.8407801418439716e-06,
      "loss": 0.7215,
      "step": 42000
    },
    {
      "epoch": 11.724137931034482,
      "grad_norm": 3.9311294555664062,
      "learning_rate": 2.7521276595744683e-06,
      "loss": 0.7188,
      "step": 42500
    },
    {
      "epoch": 11.862068965517242,
      "grad_norm": 3.821101427078247,
      "learning_rate": 2.663475177304965e-06,
      "loss": 0.721,
      "step": 43000
    },
    {
      "epoch": 12.0,
      "grad_norm": 4.8846306800842285,
      "learning_rate": 2.574822695035461e-06,
      "loss": 0.7214,
      "step": 43500
    },
    {
      "epoch": 12.137931034482758,
      "grad_norm": 4.813929557800293,
      "learning_rate": 2.4863475177304966e-06,
      "loss": 0.7129,
      "step": 44000
    },
    {
      "epoch": 12.275862068965518,
      "grad_norm": 4.029230117797852,
      "learning_rate": 2.3976950354609933e-06,
      "loss": 0.7155,
      "step": 44500
    },
    {
      "epoch": 12.413793103448276,
      "grad_norm": 4.029098987579346,
      "learning_rate": 2.3090425531914895e-06,
      "loss": 0.713,
      "step": 45000
    },
    {
      "epoch": 12.551724137931034,
      "grad_norm": 3.890687942504883,
      "learning_rate": 2.220390070921986e-06,
      "loss": 0.711,
      "step": 45500
    },
    {
      "epoch": 12.689655172413794,
      "grad_norm": 4.256452560424805,
      "learning_rate": 2.1319148936170216e-06,
      "loss": 0.7141,
      "step": 46000
    },
    {
      "epoch": 12.827586206896552,
      "grad_norm": 3.49444317817688,
      "learning_rate": 2.043262411347518e-06,
      "loss": 0.7086,
      "step": 46500
    },
    {
      "epoch": 12.96551724137931,
      "grad_norm": 3.2972991466522217,
      "learning_rate": 1.9546099290780145e-06,
      "loss": 0.7169,
      "step": 47000
    },
    {
      "epoch": 13.10344827586207,
      "grad_norm": 4.557949542999268,
      "learning_rate": 1.8659574468085107e-06,
      "loss": 0.7112,
      "step": 47500
    },
    {
      "epoch": 13.241379310344827,
      "grad_norm": 4.156935214996338,
      "learning_rate": 1.7773049645390072e-06,
      "loss": 0.7056,
      "step": 48000
    },
    {
      "epoch": 13.241379310344827,
      "eval_loss": 0.7582518458366394,
      "eval_runtime": 56.6838,
      "eval_samples_per_second": 109.661,
      "eval_steps_per_second": 27.415,
      "step": 48000
    }
  ],
  "logging_steps": 500,
  "max_steps": 58000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 16,
  "save_steps": 16000,
  "total_flos": 1.2541168336896e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
